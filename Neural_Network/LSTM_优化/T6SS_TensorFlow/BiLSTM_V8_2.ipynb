{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, ReLU, Bidirectional,BatchNormalization, Conv1D, MaxPool1D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = ['sans-serif']\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] =False\n",
    "plt.rcParams.update({\"font.size\":20})\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = pd.read_csv('data/Negative.txt', header = None, index_col=None) \n",
    "negative = negative.drop_duplicates()\n",
    "negative['label'] = 0\n",
    "positive = pd.read_csv('data/Positive.txt', header = None, index_col=None) \n",
    "positive = positive.drop_duplicates()\n",
    "positive['label'] = 1\n",
    "# negative[negative.isnull().values==True] # 判断是否存在缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([negative, positive], axis=0).reset_index().iloc[:, 1:]\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据归一化处理\n",
    "x_data = data.iloc[:, :-1]\n",
    "y_data = data.loc[:, 'label']\n",
    "mm = MinMaxScaler([-1, 1])\n",
    "x_data_mm = mm.fit_transform(x_data)\n",
    "# 数据降维\n",
    "# pca = PCA(n_components=10) \n",
    "# x_data_mm_pca = pca.fit_transform(x_data_mm)\n",
    "# 类别均衡化\n",
    "model_smote = SMOTE()  \n",
    "x_resampled, y_resampled = model_smote.fit_resample(x_data_mm,y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练数据为80%训练集和20%的验证集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1777, 1, 2000)\n"
     ]
    }
   ],
   "source": [
    "# 重置数据维度\n",
    "x_train = x_train.reshape(-1,1,x_train.shape[-1])\n",
    "x_test = x_test.reshape(-1,1,x_test.shape[-1])\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对类别数据进行onehot编码\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              multiple                  4096128   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            multiple                  131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional multiple                  98816     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  512       \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection multiple                  98816     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch multiple                  512       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  258       \n",
      "=================================================================\n",
      "Total params: 4,426,754\n",
      "Trainable params: 4,425,986\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 构建网络\n",
    "model = Sequential() \n",
    "\n",
    "model.add(Conv1D(128,16, activation='relu', padding='same'))\n",
    "model.add(Conv1D(128,8, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool1D(pool_size=5, padding='same'))\n",
    "\n",
    "\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True, activation='relu', input_dim=x_train.shape[-1], input_length=x_train.shape[1])))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(units=64)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.build(input_shape=x_train.shape)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6307 - accuracy: 0.6956\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.53708, saving model to BiLSTM_V8_3.h5\n",
      "28/28 [==============================] - 7s 233ms/step - loss: 0.6307 - accuracy: 0.6956 - val_loss: 0.7112 - val_accuracy: 0.5371 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4357 - accuracy: 0.8036\n",
      "Epoch 00002: val_accuracy did not improve from 0.53708\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.4357 - accuracy: 0.8036 - val_loss: 0.7206 - val_accuracy: 0.5169 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3049 - accuracy: 0.8739\n",
      "Epoch 00003: val_accuracy did not improve from 0.53708\n",
      "28/28 [==============================] - 5s 173ms/step - loss: 0.3049 - accuracy: 0.8739 - val_loss: 0.7042 - val_accuracy: 0.4966 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2139 - accuracy: 0.9178\n",
      "Epoch 00004: val_accuracy improved from 0.53708 to 0.64270, saving model to BiLSTM_V8_3.h5\n",
      "28/28 [==============================] - 5s 195ms/step - loss: 0.2139 - accuracy: 0.9178 - val_loss: 0.6881 - val_accuracy: 0.6427 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2041 - accuracy: 0.9133\n",
      "Epoch 00005: val_accuracy did not improve from 0.64270\n",
      "28/28 [==============================] - 5s 176ms/step - loss: 0.2041 - accuracy: 0.9133 - val_loss: 0.7058 - val_accuracy: 0.5303 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1724 - accuracy: 0.9336\n",
      "Epoch 00006: val_accuracy did not improve from 0.64270\n",
      "28/28 [==============================] - 5s 184ms/step - loss: 0.1724 - accuracy: 0.9336 - val_loss: 0.8870 - val_accuracy: 0.5371 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1645 - accuracy: 0.9398\n",
      "Epoch 00007: val_accuracy did not improve from 0.64270\n",
      "28/28 [==============================] - 5s 181ms/step - loss: 0.1645 - accuracy: 0.9398 - val_loss: 0.6413 - val_accuracy: 0.5371 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1618 - accuracy: 0.9381\n",
      "Epoch 00008: val_accuracy improved from 0.64270 to 0.66966, saving model to BiLSTM_V8_3.h5\n",
      "28/28 [==============================] - 5s 190ms/step - loss: 0.1618 - accuracy: 0.9381 - val_loss: 0.6159 - val_accuracy: 0.6697 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9747\n",
      "Epoch 00009: val_accuracy improved from 0.66966 to 0.68090, saving model to BiLSTM_V8_3.h5\n",
      "28/28 [==============================] - 5s 183ms/step - loss: 0.0813 - accuracy: 0.9747 - val_loss: 0.5548 - val_accuracy: 0.6809 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9730\n",
      "Epoch 00010: val_accuracy improved from 0.68090 to 0.85618, saving model to BiLSTM_V8_3.h5\n",
      "28/28 [==============================] - 5s 177ms/step - loss: 0.0698 - accuracy: 0.9730 - val_loss: 0.5003 - val_accuracy: 0.8562 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0567 - accuracy: 0.9809\n",
      "Epoch 00011: val_accuracy did not improve from 0.85618\n",
      "28/28 [==============================] - 6s 206ms/step - loss: 0.0567 - accuracy: 0.9809 - val_loss: 0.5381 - val_accuracy: 0.7281 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9634\n",
      "Epoch 00012: val_accuracy did not improve from 0.85618\n",
      "28/28 [==============================] - 5s 191ms/step - loss: 0.0873 - accuracy: 0.9634 - val_loss: 0.7516 - val_accuracy: 0.6022 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0622 - accuracy: 0.9758\n",
      "Epoch 00013: val_accuracy did not improve from 0.85618\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 0.0622 - accuracy: 0.9758 - val_loss: 1.2024 - val_accuracy: 0.6045 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9876\n",
      "Epoch 00014: val_accuracy did not improve from 0.85618\n",
      "28/28 [==============================] - 6s 223ms/step - loss: 0.0323 - accuracy: 0.9876 - val_loss: 1.3397 - val_accuracy: 0.5371 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9814\n",
      "Epoch 00015: val_accuracy did not improve from 0.85618\n",
      "28/28 [==============================] - 7s 232ms/step - loss: 0.0443 - accuracy: 0.9814 - val_loss: 1.6419 - val_accuracy: 0.5483 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9769\n",
      "Epoch 00016: val_accuracy did not improve from 0.85618\n",
      "28/28 [==============================] - 6s 228ms/step - loss: 0.0656 - accuracy: 0.9769 - val_loss: 0.9253 - val_accuracy: 0.6225 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9809\n",
      "Epoch 00017: val_accuracy did not improve from 0.85618\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 0.0578 - accuracy: 0.9809 - val_loss: 1.5262 - val_accuracy: 0.5730 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9871\n",
      "Epoch 00018: val_accuracy did not improve from 0.85618\n",
      "28/28 [==============================] - 6s 219ms/step - loss: 0.0396 - accuracy: 0.9871 - val_loss: 4.0679 - val_accuracy: 0.5393 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9899\n",
      "Epoch 00019: val_accuracy did not improve from 0.85618\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 0.0305 - accuracy: 0.9899 - val_loss: 4.6935 - val_accuracy: 0.5371 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9893\n",
      "Epoch 00020: val_accuracy did not improve from 0.85618\n",
      "28/28 [==============================] - 6s 221ms/step - loss: 0.0295 - accuracy: 0.9893 - val_loss: 7.8174 - val_accuracy: 0.5371 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9820\n",
      "Epoch 00021: val_accuracy did not improve from 0.85618\n",
      "28/28 [==============================] - 6s 220ms/step - loss: 0.0559 - accuracy: 0.9820 - val_loss: 3.1032 - val_accuracy: 0.5393 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0555 - accuracy: 0.9820\n",
      "Epoch 00022: val_accuracy did not improve from 0.85618\n",
      "28/28 [==============================] - 6s 221ms/step - loss: 0.0555 - accuracy: 0.9820 - val_loss: 1.9171 - val_accuracy: 0.6090 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9831\n",
      "Epoch 00023: val_accuracy did not improve from 0.85618\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 0.0409 - accuracy: 0.9831 - val_loss: 1.5820 - val_accuracy: 0.6562 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9932\n",
      "Epoch 00024: val_accuracy did not improve from 0.85618\n",
      "28/28 [==============================] - 7s 245ms/step - loss: 0.0239 - accuracy: 0.9932 - val_loss: 0.6421 - val_accuracy: 0.7079 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9848\n",
      "Epoch 00025: val_accuracy did not improve from 0.85618\n",
      "28/28 [==============================] - 6s 231ms/step - loss: 0.0381 - accuracy: 0.9848 - val_loss: 0.9507 - val_accuracy: 0.7438 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9859\n",
      "Epoch 00026: val_accuracy did not improve from 0.85618\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 0.0294 - accuracy: 0.9859 - val_loss: 0.8899 - val_accuracy: 0.7483 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9887\n",
      "Epoch 00027: val_accuracy improved from 0.85618 to 0.87640, saving model to BiLSTM_V8_3.h5\n",
      "28/28 [==============================] - 6s 224ms/step - loss: 0.0337 - accuracy: 0.9887 - val_loss: 0.6408 - val_accuracy: 0.8764 - lr: 0.0010\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9893\n",
      "Epoch 00028: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 219ms/step - loss: 0.0351 - accuracy: 0.9893 - val_loss: 1.8103 - val_accuracy: 0.6944 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9887\n",
      "Epoch 00029: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 222ms/step - loss: 0.0241 - accuracy: 0.9887 - val_loss: 2.7785 - val_accuracy: 0.5663 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9921\n",
      "Epoch 00030: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 7s 241ms/step - loss: 0.0180 - accuracy: 0.9921 - val_loss: 0.5757 - val_accuracy: 0.8067 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9966\n",
      "Epoch 00031: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 222ms/step - loss: 0.0098 - accuracy: 0.9966 - val_loss: 0.8841 - val_accuracy: 0.7820 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.9989\n",
      "Epoch 00032: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 222ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.9365 - val_accuracy: 0.7865 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9938\n",
      "Epoch 00033: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 222ms/step - loss: 0.0212 - accuracy: 0.9938 - val_loss: 0.8096 - val_accuracy: 0.7573 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9899\n",
      "Epoch 00034: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 222ms/step - loss: 0.0288 - accuracy: 0.9899 - val_loss: 6.7715 - val_accuracy: 0.5416 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9955\n",
      "Epoch 00035: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 0.0161 - accuracy: 0.9955 - val_loss: 1.1122 - val_accuracy: 0.7596 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9949\n",
      "Epoch 00036: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 0.0201 - accuracy: 0.9949 - val_loss: 0.7312 - val_accuracy: 0.8247 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9921\n",
      "Epoch 00037: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 0.0276 - accuracy: 0.9921 - val_loss: 2.2161 - val_accuracy: 0.7034 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9977\n",
      "Epoch 00038: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 0.0096 - accuracy: 0.9977 - val_loss: 2.9697 - val_accuracy: 0.7169 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 00039: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 1.8187 - val_accuracy: 0.7483 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 0.9989\n",
      "Epoch 00040: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 222ms/step - loss: 0.0052 - accuracy: 0.9989 - val_loss: 0.8060 - val_accuracy: 0.7910 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 0.9989\n",
      "Epoch 00041: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 220ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 2.4781 - val_accuracy: 0.7281 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 0.9994\n",
      "Epoch 00042: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 220ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.7411 - val_accuracy: 0.8157 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9966\n",
      "Epoch 00043: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 229ms/step - loss: 0.0088 - accuracy: 0.9966 - val_loss: 0.9059 - val_accuracy: 0.8360 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9932\n",
      "Epoch 00044: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 221ms/step - loss: 0.0194 - accuracy: 0.9932 - val_loss: 1.3596 - val_accuracy: 0.6202 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9966\n",
      "Epoch 00045: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 215ms/step - loss: 0.0137 - accuracy: 0.9966 - val_loss: 2.6250 - val_accuracy: 0.5955 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9972\n",
      "Epoch 00046: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 220ms/step - loss: 0.0100 - accuracy: 0.9972 - val_loss: 2.5438 - val_accuracy: 0.5775 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9949\n",
      "Epoch 00047: val_accuracy did not improve from 0.87640\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 0.0177 - accuracy: 0.9949 - val_loss: 3.4498 - val_accuracy: 0.5910 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9989\n",
      "Epoch 00048: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 219ms/step - loss: 0.0065 - accuracy: 0.9989 - val_loss: 2.6023 - val_accuracy: 0.5775 - lr: 5.0000e-04\n",
      "Epoch 49/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 00049: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 220ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 1.4805 - val_accuracy: 0.6876 - lr: 5.0000e-04\n",
      "Epoch 50/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.9994\n",
      "Epoch 00050: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 215ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.9106 - val_accuracy: 0.7438 - lr: 5.0000e-04\n",
      "Epoch 51/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 00051: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6131 - val_accuracy: 0.8607 - lr: 5.0000e-04\n",
      "Epoch 52/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00052: val_accuracy did not improve from 0.87640\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.6087 - val_accuracy: 0.8764 - lr: 5.0000e-04\n",
      "Epoch 53/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 5.8664e-04 - accuracy: 1.0000\n",
      "Epoch 00053: val_accuracy improved from 0.87640 to 0.89888, saving model to BiLSTM_V8_3.h5\n",
      "28/28 [==============================] - 6s 226ms/step - loss: 5.8664e-04 - accuracy: 1.0000 - val_loss: 0.5819 - val_accuracy: 0.8989 - lr: 5.0000e-04\n",
      "Epoch 54/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 9.9708e-04 - accuracy: 1.0000\n",
      "Epoch 00054: val_accuracy did not improve from 0.89888\n",
      "28/28 [==============================] - 6s 220ms/step - loss: 9.9708e-04 - accuracy: 1.0000 - val_loss: 0.5943 - val_accuracy: 0.8899 - lr: 5.0000e-04\n",
      "Epoch 55/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 00055: val_accuracy did not improve from 0.89888\n",
      "28/28 [==============================] - 6s 221ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.9335 - val_accuracy: 0.8764 - lr: 5.0000e-04\n",
      "Epoch 56/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00056: val_accuracy improved from 0.89888 to 0.90787, saving model to BiLSTM_V8_3.h5\n",
      "28/28 [==============================] - 6s 228ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.6634 - val_accuracy: 0.9079 - lr: 5.0000e-04\n",
      "Epoch 57/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 6.1629e-04 - accuracy: 1.0000\n",
      "Epoch 00057: val_accuracy improved from 0.90787 to 0.91461, saving model to BiLSTM_V8_3.h5\n",
      "28/28 [==============================] - 6s 226ms/step - loss: 6.1629e-04 - accuracy: 1.0000 - val_loss: 0.5206 - val_accuracy: 0.9146 - lr: 5.0000e-04\n",
      "Epoch 58/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9977\n",
      "Epoch 00058: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 0.0067 - accuracy: 0.9977 - val_loss: 1.2115 - val_accuracy: 0.7483 - lr: 5.0000e-04\n",
      "Epoch 59/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.9983\n",
      "Epoch 00059: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 214ms/step - loss: 0.0042 - accuracy: 0.9983 - val_loss: 2.4822 - val_accuracy: 0.6629 - lr: 5.0000e-04\n",
      "Epoch 60/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00060: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.0225 - val_accuracy: 0.8022 - lr: 5.0000e-04\n",
      "Epoch 61/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00061: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.9674 - val_accuracy: 0.8022 - lr: 5.0000e-04\n",
      "Epoch 62/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 6.0511e-04 - accuracy: 1.0000\n",
      "Epoch 00062: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 222ms/step - loss: 6.0511e-04 - accuracy: 1.0000 - val_loss: 0.6977 - val_accuracy: 0.8719 - lr: 5.0000e-04\n",
      "Epoch 63/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 6.8568e-04 - accuracy: 1.0000\n",
      "Epoch 00063: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 215ms/step - loss: 6.8568e-04 - accuracy: 1.0000 - val_loss: 0.7233 - val_accuracy: 0.8270 - lr: 5.0000e-04\n",
      "Epoch 64/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 7.5355e-04 - accuracy: 1.0000\n",
      "Epoch 00064: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 7.5355e-04 - accuracy: 1.0000 - val_loss: 0.5263 - val_accuracy: 0.8854 - lr: 5.0000e-04\n",
      "Epoch 65/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 00065: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 222ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.9990 - val_accuracy: 0.8449 - lr: 5.0000e-04\n",
      "Epoch 66/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 7.4089e-04 - accuracy: 1.0000\n",
      "Epoch 00066: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 7.4089e-04 - accuracy: 1.0000 - val_loss: 1.2541 - val_accuracy: 0.8427 - lr: 5.0000e-04\n",
      "Epoch 67/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 3.3683e-04 - accuracy: 1.0000\n",
      "Epoch 00067: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 224ms/step - loss: 3.3683e-04 - accuracy: 1.0000 - val_loss: 0.7383 - val_accuracy: 0.9056 - lr: 5.0000e-04\n",
      "Epoch 68/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 00068: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 223ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5932 - val_accuracy: 0.8854 - lr: 5.0000e-04\n",
      "Epoch 69/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 6.7228e-04 - accuracy: 1.0000\n",
      "Epoch 00069: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 6.7228e-04 - accuracy: 1.0000 - val_loss: 0.4715 - val_accuracy: 0.8854 - lr: 5.0000e-04\n",
      "Epoch 70/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9994\n",
      "Epoch 00070: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 215ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 1.1723 - val_accuracy: 0.7056 - lr: 5.0000e-04\n",
      "Epoch 71/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 7.5765e-04 - accuracy: 1.0000\n",
      "Epoch 00071: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 223ms/step - loss: 7.5765e-04 - accuracy: 1.0000 - val_loss: 1.3822 - val_accuracy: 0.7034 - lr: 5.0000e-04\n",
      "Epoch 72/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 00072: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6647 - val_accuracy: 0.9079 - lr: 5.0000e-04\n",
      "Epoch 73/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9961\n",
      "Epoch 00073: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 0.0086 - accuracy: 0.9961 - val_loss: 0.8193 - val_accuracy: 0.8067 - lr: 5.0000e-04\n",
      "Epoch 74/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 0.9989\n",
      "Epoch 00074: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 223ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 5.6529 - val_accuracy: 0.6180 - lr: 5.0000e-04\n",
      "Epoch 75/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9983\n",
      "Epoch 00075: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 212ms/step - loss: 0.0080 - accuracy: 0.9983 - val_loss: 1.7604 - val_accuracy: 0.7461 - lr: 5.0000e-04\n",
      "Epoch 76/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 0.9977\n",
      "Epoch 00076: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 0.0057 - accuracy: 0.9977 - val_loss: 1.3638 - val_accuracy: 0.7685 - lr: 5.0000e-04\n",
      "Epoch 77/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9994\n",
      "Epoch 00077: val_accuracy did not improve from 0.91461\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "28/28 [==============================] - 6s 215ms/step - loss: 0.0013 - accuracy: 0.9994 - val_loss: 1.2332 - val_accuracy: 0.7910 - lr: 5.0000e-04\n",
      "Epoch 78/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 8.0736e-04 - accuracy: 1.0000\n",
      "Epoch 00078: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 215ms/step - loss: 8.0736e-04 - accuracy: 1.0000 - val_loss: 0.6366 - val_accuracy: 0.8809 - lr: 2.5000e-04\n",
      "Epoch 79/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 5.1003e-04 - accuracy: 1.0000\n",
      "Epoch 00079: val_accuracy did not improve from 0.91461\n",
      "28/28 [==============================] - 6s 224ms/step - loss: 5.1003e-04 - accuracy: 1.0000 - val_loss: 0.5207 - val_accuracy: 0.9101 - lr: 2.5000e-04\n",
      "Epoch 80/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 5.3773e-04 - accuracy: 1.0000\n",
      "Epoch 00080: val_accuracy improved from 0.91461 to 0.91910, saving model to BiLSTM_V8_3.h5\n",
      "28/28 [==============================] - 6s 222ms/step - loss: 5.3773e-04 - accuracy: 1.0000 - val_loss: 0.5011 - val_accuracy: 0.9191 - lr: 2.5000e-04\n",
      "Epoch 81/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 9.8305e-04 - accuracy: 1.0000\n",
      "Epoch 00081: val_accuracy improved from 0.91910 to 0.93708, saving model to BiLSTM_V8_3.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 6s 224ms/step - loss: 9.8305e-04 - accuracy: 1.0000 - val_loss: 0.4184 - val_accuracy: 0.9371 - lr: 2.5000e-04\n",
      "Epoch 82/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 8.6247e-04 - accuracy: 1.0000\n",
      "Epoch 00082: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 8.6247e-04 - accuracy: 1.0000 - val_loss: 0.4477 - val_accuracy: 0.9258 - lr: 2.5000e-04\n",
      "Epoch 83/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00083: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 212ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4468 - val_accuracy: 0.9258 - lr: 2.5000e-04\n",
      "Epoch 84/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 3.9588e-04 - accuracy: 1.0000\n",
      "Epoch 00084: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 3.9588e-04 - accuracy: 1.0000 - val_loss: 0.4542 - val_accuracy: 0.9303 - lr: 2.5000e-04\n",
      "Epoch 85/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 2.8035e-04 - accuracy: 1.0000\n",
      "Epoch 00085: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 214ms/step - loss: 2.8035e-04 - accuracy: 1.0000 - val_loss: 0.4446 - val_accuracy: 0.9281 - lr: 2.5000e-04\n",
      "Epoch 86/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 9.1880e-04 - accuracy: 1.0000\n",
      "Epoch 00086: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 9.1880e-04 - accuracy: 1.0000 - val_loss: 0.4543 - val_accuracy: 0.9348 - lr: 2.5000e-04\n",
      "Epoch 87/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.9989\n",
      "Epoch 00087: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 220ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.5213 - val_accuracy: 0.9169 - lr: 2.5000e-04\n",
      "Epoch 88/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 00088: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4995 - val_accuracy: 0.8876 - lr: 2.5000e-04\n",
      "Epoch 89/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 6.6159e-04 - accuracy: 1.0000\n",
      "Epoch 00089: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 6.6159e-04 - accuracy: 1.0000 - val_loss: 0.6271 - val_accuracy: 0.9079 - lr: 2.5000e-04\n",
      "Epoch 90/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 6.4888e-04 - accuracy: 1.0000\n",
      "Epoch 00090: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 6.4888e-04 - accuracy: 1.0000 - val_loss: 0.5865 - val_accuracy: 0.9101 - lr: 2.5000e-04\n",
      "Epoch 91/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 5.6072e-04 - accuracy: 1.0000\n",
      "Epoch 00091: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 219ms/step - loss: 5.6072e-04 - accuracy: 1.0000 - val_loss: 0.4621 - val_accuracy: 0.9213 - lr: 2.5000e-04\n",
      "Epoch 92/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00092: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 214ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4247 - val_accuracy: 0.9236 - lr: 2.5000e-04\n",
      "Epoch 93/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 5.7051e-04 - accuracy: 1.0000\n",
      "Epoch 00093: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 220ms/step - loss: 5.7051e-04 - accuracy: 1.0000 - val_loss: 0.4268 - val_accuracy: 0.9348 - lr: 2.5000e-04\n",
      "Epoch 94/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 4.0381e-04 - accuracy: 1.0000\n",
      "Epoch 00094: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 215ms/step - loss: 4.0381e-04 - accuracy: 1.0000 - val_loss: 0.4275 - val_accuracy: 0.9371 - lr: 2.5000e-04\n",
      "Epoch 95/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 3.7385e-04 - accuracy: 1.0000\n",
      "Epoch 00095: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 3.7385e-04 - accuracy: 1.0000 - val_loss: 0.4800 - val_accuracy: 0.9303 - lr: 2.5000e-04\n",
      "Epoch 96/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.6903e-04 - accuracy: 1.0000\n",
      "Epoch 00096: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 215ms/step - loss: 1.6903e-04 - accuracy: 1.0000 - val_loss: 0.5006 - val_accuracy: 0.9303 - lr: 2.5000e-04\n",
      "Epoch 97/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 6.8784e-04 - accuracy: 1.0000\n",
      "Epoch 00097: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 6.8784e-04 - accuracy: 1.0000 - val_loss: 0.5891 - val_accuracy: 0.9191 - lr: 2.5000e-04\n",
      "Epoch 98/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 3.7469e-04 - accuracy: 1.0000\n",
      "Epoch 00098: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 3.7469e-04 - accuracy: 1.0000 - val_loss: 0.5476 - val_accuracy: 0.9191 - lr: 2.5000e-04\n",
      "Epoch 99/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 3.0520e-04 - accuracy: 1.0000\n",
      "Epoch 00099: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 219ms/step - loss: 3.0520e-04 - accuracy: 1.0000 - val_loss: 0.5044 - val_accuracy: 0.9258 - lr: 2.5000e-04\n",
      "Epoch 100/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.2295e-04 - accuracy: 1.0000\n",
      "Epoch 00100: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 224ms/step - loss: 1.2295e-04 - accuracy: 1.0000 - val_loss: 0.4953 - val_accuracy: 0.9281 - lr: 2.5000e-04\n",
      "Epoch 101/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 4.6362e-04 - accuracy: 1.0000\n",
      "Epoch 00101: val_accuracy did not improve from 0.93708\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "28/28 [==============================] - 6s 222ms/step - loss: 4.6362e-04 - accuracy: 1.0000 - val_loss: 0.9881 - val_accuracy: 0.8202 - lr: 2.5000e-04\n",
      "Epoch 102/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9994\n",
      "Epoch 00102: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 225ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.4563 - val_accuracy: 0.9169 - lr: 1.2500e-04\n",
      "Epoch 103/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 2.4108e-04 - accuracy: 1.0000\n",
      "Epoch 00103: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 2.4108e-04 - accuracy: 1.0000 - val_loss: 0.4769 - val_accuracy: 0.9236 - lr: 1.2500e-04\n",
      "Epoch 104/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 2.3159e-04 - accuracy: 1.0000\n",
      "Epoch 00104: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 2.3159e-04 - accuracy: 1.0000 - val_loss: 0.4530 - val_accuracy: 0.9303 - lr: 1.2500e-04\n",
      "Epoch 105/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 5.9199e-04 - accuracy: 1.0000\n",
      "Epoch 00105: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 213ms/step - loss: 5.9199e-04 - accuracy: 1.0000 - val_loss: 0.4419 - val_accuracy: 0.9281 - lr: 1.2500e-04\n",
      "Epoch 106/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.6051e-04 - accuracy: 1.0000\n",
      "Epoch 00106: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 1.6051e-04 - accuracy: 1.0000 - val_loss: 0.4429 - val_accuracy: 0.9371 - lr: 1.2500e-04\n",
      "Epoch 107/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 2.3474e-04 - accuracy: 1.0000\n",
      "Epoch 00107: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 2.3474e-04 - accuracy: 1.0000 - val_loss: 0.4661 - val_accuracy: 0.9326 - lr: 1.2500e-04\n",
      "Epoch 108/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - ETA: 0s - loss: 4.5280e-04 - accuracy: 1.0000\n",
      "Epoch 00108: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 4.5280e-04 - accuracy: 1.0000 - val_loss: 0.4873 - val_accuracy: 0.9303 - lr: 1.2500e-04\n",
      "Epoch 109/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 4.4532e-04 - accuracy: 1.0000\n",
      "Epoch 00109: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 4.4532e-04 - accuracy: 1.0000 - val_loss: 0.5448 - val_accuracy: 0.9169 - lr: 1.2500e-04\n",
      "Epoch 110/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 3.3291e-04 - accuracy: 1.0000\n",
      "Epoch 00110: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 3.3291e-04 - accuracy: 1.0000 - val_loss: 0.5510 - val_accuracy: 0.9236 - lr: 1.2500e-04\n",
      "Epoch 111/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 2.0774e-04 - accuracy: 1.0000\n",
      "Epoch 00111: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 2.0774e-04 - accuracy: 1.0000 - val_loss: 0.5859 - val_accuracy: 0.9191 - lr: 1.2500e-04\n",
      "Epoch 112/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.2289e-04 - accuracy: 1.0000\n",
      "Epoch 00112: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 1.2289e-04 - accuracy: 1.0000 - val_loss: 0.5694 - val_accuracy: 0.9169 - lr: 1.2500e-04\n",
      "Epoch 113/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 5.3261e-04 - accuracy: 1.0000\n",
      "Epoch 00113: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 214ms/step - loss: 5.3261e-04 - accuracy: 1.0000 - val_loss: 0.5919 - val_accuracy: 0.9191 - lr: 1.2500e-04\n",
      "Epoch 114/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 4.2705e-04 - accuracy: 1.0000\n",
      "Epoch 00114: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 225ms/step - loss: 4.2705e-04 - accuracy: 1.0000 - val_loss: 0.5779 - val_accuracy: 0.9213 - lr: 1.2500e-04\n",
      "Epoch 115/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 7.8084e-05 - accuracy: 1.0000\n",
      "Epoch 00115: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 7.8084e-05 - accuracy: 1.0000 - val_loss: 0.5570 - val_accuracy: 0.9236 - lr: 1.2500e-04\n",
      "Epoch 116/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 2.1479e-04 - accuracy: 1.0000\n",
      "Epoch 00116: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 2.1479e-04 - accuracy: 1.0000 - val_loss: 0.5509 - val_accuracy: 0.9236 - lr: 1.2500e-04\n",
      "Epoch 117/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 4.5358e-04 - accuracy: 1.0000\n",
      "Epoch 00117: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 4.5358e-04 - accuracy: 1.0000 - val_loss: 0.6011 - val_accuracy: 0.9213 - lr: 1.2500e-04\n",
      "Epoch 118/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 2.4586e-04 - accuracy: 1.0000\n",
      "Epoch 00118: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 222ms/step - loss: 2.4586e-04 - accuracy: 1.0000 - val_loss: 0.6446 - val_accuracy: 0.9191 - lr: 1.2500e-04\n",
      "Epoch 119/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 9.7694e-04 - accuracy: 0.9994\n",
      "Epoch 00119: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 227ms/step - loss: 9.7694e-04 - accuracy: 0.9994 - val_loss: 0.6828 - val_accuracy: 0.8292 - lr: 1.2500e-04\n",
      "Epoch 120/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.4934e-04 - accuracy: 1.0000\n",
      "Epoch 00120: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 223ms/step - loss: 1.4934e-04 - accuracy: 1.0000 - val_loss: 0.4520 - val_accuracy: 0.9056 - lr: 1.2500e-04\n",
      "Epoch 121/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 3.2636e-04 - accuracy: 1.0000\n",
      "Epoch 00121: val_accuracy did not improve from 0.93708\n",
      "\n",
      "Epoch 00121: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "28/28 [==============================] - 6s 214ms/step - loss: 3.2636e-04 - accuracy: 1.0000 - val_loss: 0.4077 - val_accuracy: 0.9303 - lr: 1.2500e-04\n",
      "Epoch 122/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.8767e-04 - accuracy: 1.0000\n",
      "Epoch 00122: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 1.8767e-04 - accuracy: 1.0000 - val_loss: 0.4282 - val_accuracy: 0.9281 - lr: 6.2500e-05\n",
      "Epoch 123/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 2.2269e-04 - accuracy: 1.0000\n",
      "Epoch 00123: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 2.2269e-04 - accuracy: 1.0000 - val_loss: 0.4464 - val_accuracy: 0.9281 - lr: 6.2500e-05\n",
      "Epoch 124/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 6.8531e-04 - accuracy: 1.0000\n",
      "Epoch 00124: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 7s 243ms/step - loss: 6.8531e-04 - accuracy: 1.0000 - val_loss: 0.4617 - val_accuracy: 0.9281 - lr: 6.2500e-05\n",
      "Epoch 125/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.0119e-04 - accuracy: 1.0000\n",
      "Epoch 00125: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 7s 244ms/step - loss: 1.0119e-04 - accuracy: 1.0000 - val_loss: 0.4865 - val_accuracy: 0.9303 - lr: 6.2500e-05\n",
      "Epoch 126/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.8902e-04 - accuracy: 1.0000\n",
      "Epoch 00126: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 7s 240ms/step - loss: 1.8902e-04 - accuracy: 1.0000 - val_loss: 0.4867 - val_accuracy: 0.9303 - lr: 6.2500e-05\n",
      "Epoch 127/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 9.7672e-05 - accuracy: 1.0000\n",
      "Epoch 00127: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 7s 249ms/step - loss: 9.7672e-05 - accuracy: 1.0000 - val_loss: 0.4900 - val_accuracy: 0.9326 - lr: 6.2500e-05\n",
      "Epoch 128/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 2.4979e-04 - accuracy: 1.0000\n",
      "Epoch 00128: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 7s 252ms/step - loss: 2.4979e-04 - accuracy: 1.0000 - val_loss: 0.5142 - val_accuracy: 0.9303 - lr: 6.2500e-05\n",
      "Epoch 129/200\n",
      "28/28 [==============================] - ETA: 0s - loss: 3.6180e-04 - accuracy: 1.0000\n",
      "Epoch 00129: val_accuracy did not improve from 0.93708\n",
      "28/28 [==============================] - 6s 229ms/step - loss: 3.6180e-04 - accuracy: 1.0000 - val_loss: 0.5110 - val_accuracy: 0.9281 - lr: 6.2500e-05\n",
      "Epoch 130/200\n",
      "21/28 [=====================>........] - ETA: 1s - loss: 3.3822e-04 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "# 训练网络\n",
    "epochs = 200 # 训练周期\n",
    "batch_size = 64 # 批次大小 \n",
    "# 监视网络的训练，并将在验证集上取得最优结果的网络权重保留下来\n",
    "checkpoint = ModelCheckpoint(\"BiLSTM_V8_3.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='max') \n",
    "# 监视网络的训练，并对学习率进行衰减处理\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=20, verbose=1)\n",
    "callbacks_list = [checkpoint, reduce_lr] # 监视列表 \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # 编译网络\n",
    "# 开始训练，直接使用30%的验证集，在训练过程中测试网络\n",
    "history = model.fit(x_train, y_train, callbacks=callbacks_list, epochs=epochs, batch_size=batch_size, verbose=1,validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化训练过程中，训练集与验证集的损失和精度\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'],label = 'train_loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['accuracy'],label = 'train_accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用模型得到预测结果\n",
    "model = load_model('BiLSTM_V8_3.h5')\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('在80%训练集上, accuracy: {:.4}% 召回率: {:4}, f1: {:4}'.format(accuracy_score(np.argmax(y_train, axis = 1), np.argmax(y_train_pred, axis = 1)), recall_score(np.argmax(y_train, axis = 1), np.argmax(y_train_pred, axis = 1), average='binary'), f1_score(np.argmax(y_train, axis = 1), np.argmax(y_train_pred, axis = 1), average='binary')))\n",
    "print('在20%测试集上, accuracy: {:.4}% 召回率: {:4}, f1: {:4}'.format(accuracy_score(np.argmax(y_test, axis = 1), np.argmax(y_test_pred, axis = 1)), recall_score(np.argmax(y_test, axis = 1), np.argmax(y_test_pred, axis = 1), average='binary'), f1_score(np.argmax(y_test, axis = 1), np.argmax(y_test_pred, axis = 1), average='binary')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制混淆矩阵\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(121)\n",
    "matrix = confusion_matrix(np.argmax(y_train, axis = 1), np.argmax(y_train_pred, axis = 1))\n",
    "sns.heatmap(matrix,annot=True,cmap=\"Blues\",fmt='g') \n",
    "plt.title('Train confusion matrix') \n",
    "plt.xticks(range(2), ['Negative', 'Positive'])\n",
    "plt.yticks(range(2), ['Negative', 'Positive'])\n",
    "plt.xlabel('Predict') \n",
    "plt.ylabel('True')\n",
    "plt.subplot(122)\n",
    "matrix = confusion_matrix(np.argmax(y_test, axis = 1), np.argmax(y_test_pred, axis = 1))\n",
    "sns.heatmap(matrix,annot=True,cmap=\"Blues\",fmt='g') \n",
    "plt.title('Test confusion matrix') \n",
    "plt.xticks(range(2), ['Negative', 'Positive'])\n",
    "plt.yticks(range(2), ['Negative', 'Positive'])\n",
    "plt.xlabel('Predict') \n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
